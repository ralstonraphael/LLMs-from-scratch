{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T23:23:49.163704Z",
     "start_time": "2025-04-23T23:23:49.128338Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "temperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "for i, probas in enumerate(scaled_probas):\n",
    "    print(\"\\n\\nTemperature:\", temperatures[i])\n",
    "    print_sampled_tokens(probas)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Temperature: 1\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "\n",
      "\n",
      "Temperature: 0.1\n",
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "\n",
      "\n",
      "Temperature: 5\n",
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pizza is sampled more often with a higher temperature, while \"moves\" is sampled more often with a lower temperature. This is because the model is more confident about the next token when the temperature is low, leading to less randomness in the sampling process. Conversely, a higher temperature introduces more randomness, allowing for a wider variety of tokens to be sampled. A faster way to determine how often the word pizza is sampled is to use the torch.bincount function, which counts the number of occurrences of each token in the sampled list. This is done by creating a tensor from the sampled list and passing it to torch.bincount. The resulting tensor contains the frequency of each token, which can then be printed out.",
   "id": "bb30c83262ae5be2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c981ef40badc48b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tuning both the temperature and top-k parameters depends on the specific language model you're using—it often requires some experimentation to get the best results.\n",
    "\n",
    "What counts as a “good” result also varies depending on the use case:\n",
    "\n",
    "Lower values for temperature and top-k make the model’s output more focused and predictable, which is ideal for tasks like writing technical content, educational material, code, or performing data analysis.\n",
    "\n",
    "Higher values lead to more varied and creative responses, which is better suited for brainstorming, storytelling, or other imaginative work."
   ],
   "id": "e0f18bbebc09830d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:51:01.980903Z",
     "start_time": "2025-04-23T23:51:00.807914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,       # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,       # Embedding dimension\n",
    "    \"n_heads\": 12,        # Number of attention heads\n",
    "    \"n_layers\": 12,       # Number of layers\n",
    "    \"drop_rate\": 0.1,     # Dropout rate\n",
    "    \"qkv_bias\": False     # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model.eval();"
   ],
   "id": "cf52e589d658ae8e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ralstonraphael/Documents/Development/AI_LLM_From_Scratch/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GPTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m123\u001B[39m)\n\u001B[1;32m     18\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m tiktoken\u001B[38;5;241m.\u001B[39mget_encoding(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 19\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mGPTModel\u001B[49m(GPT_CONFIG_124M)\n\u001B[1;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m, weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m     21\u001B[0m model\u001B[38;5;241m.\u001B[39meval();\n",
      "\u001B[0;31mNameError\u001B[0m: name 'GPTModel' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:51:57.724653Z",
     "start_time": "2025-04-23T23:51:57.713277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "from previous_chapters import generate_text_simple"
   ],
   "id": "538172e6ac1394c1",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_generate'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgpt_generate\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m generate, text_to_token_ids, token_ids_to_text\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mprevious_chapters\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m generate_text_simple\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'gpt_generate'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T23:52:16.921094Z",
     "start_time": "2025-04-23T23:52:16.910387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deterministic function that used torch.argmax\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "a76b54e0537e0a82",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_text_simple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Deterministic function that used torch.argmax\u001B[39;00m\n\u001B[1;32m      3\u001B[0m start_context \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvery effort moves you\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 5\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_text_simple\u001B[49m(\n\u001B[1;32m      6\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m      7\u001B[0m     idx\u001B[38;5;241m=\u001B[39mtext_to_token_ids(start_context, tokenizer),\n\u001B[1;32m      8\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25\u001B[39m,\n\u001B[1;32m      9\u001B[0m     context_size\u001B[38;5;241m=\u001B[39mGPT_CONFIG_124M[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext_length\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutput text:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'generate_text_simple' is not defined"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
